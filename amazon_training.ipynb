{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#os.environ[\"TORCH_USE_CUDA_DSA\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correctly setting the environment variable\n",
    "#os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from importlib.metadata import version\n",
    "\n",
    "pkgs = [\n",
    "    \"matplotlib\",  # Plotting library\n",
    "    \"tiktoken\",    # Tokenizer\n",
    "    \"torch\",       # Deep learning library\n",
    "    \"tqdm\",        # Progress bar\n",
    "    \"tensorflow\",  # For OpenAI's pretrained weights\n",
    "]\n",
    "for p in pkgs:\n",
    "    print(f\"{p} version: {version(p)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tiktoken\n",
    "from functools import partial\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import psutil\n",
    "\n",
    "import random\n",
    "from datetime import datetime\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt_reviews_finetuning import (\n",
    "    generate,\n",
    "    text_to_token_ids,\n",
    "    token_ids_to_text,\n",
    "    calc_loss_loader,\n",
    "    train_model_simple,\n",
    "    plot_losses,\n",
    "\n",
    "    GPTModel, \n",
    "    load_weights_into_gpt\n",
    ")\n",
    "\n",
    "from gpt_download import download_and_load_gpt2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_RAW_DATA = \"data/raw/Appliances.jsonl\"\n",
    "PATH_META_DATA = \"data/raw/meta_Appliances.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECT_USER_TEMPLATE = \"Phi3\" # Options: [\"Alpaca\", \"Phi3\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json_data(file_path):\n",
    "    \"\"\"\n",
    "    The function gets a file path, opens it and organizes it in a list of dictionaries.\n",
    "    :param file_path: path to a Json file with multiple Json objects.\n",
    "    :return: list of dictionaries.\n",
    "    \"\"\"\n",
    "    json_list = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for json_object in file:\n",
    "            json_dict = json.loads(json_object.strip())\n",
    "            json_list.append(json_dict)\n",
    "\n",
    "    return json_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cast_unix_to_date(unix_time_str: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Converts a Series of Unix timestamps (milliseconds) to datetime strings.\n",
    "\n",
    "    Args:\n",
    "        unix_time_str (pd.Series): Series of Unix timestamps as strings.\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: Series of datetime strings in the format 'YYYY-MM-DD'.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert to numeric directly\n",
    "    unix_timestamps = pd.to_numeric(unix_time_str)\n",
    "\n",
    "    # Convert to datetime using NumPy for efficiency\n",
    "    datetime_values = pd.to_datetime(unix_timestamps / 1000, unit='s')  # Convert milliseconds to seconds\n",
    "\n",
    "    # Format as strings\n",
    "    return datetime_values.dt.strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_input_alpaca(entry):\n",
    "    \"\"\" \n",
    "    Format entry according to the Alpaca-style prompt template. Example of data entry is as follows:\n",
    "\n",
    "    Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "    ### Instruction:\n",
    "    Identify the correct spelling of the following word.\n",
    "\n",
    "    ### Input:\n",
    "    Occassion\n",
    "\n",
    "    ### Response:\n",
    "    The correct spelling is 'Occasion.'\n",
    "    \"\"\"\n",
    "    \n",
    "    instruction_text = (\n",
    "        f\"Below is an instruction that describes a task. \"\n",
    "        f\"Write a response that appropriately completes the request.\"\n",
    "        f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n",
    "    )\n",
    "\n",
    "    input_text = f\"\\n\\n### Input:\\n{entry['input']}\" if entry[\"input\"] else \"\"\n",
    "\n",
    "    return instruction_text + input_text\n",
    "    \n",
    "\n",
    "def format_input_phi(entry):\n",
    "    \"\"\" \n",
    "    Phi-3 prompt template\n",
    "    This prompt template is substantially shorter, which reduces the runtime and hardware requirements for finetuning the LLM and generating text since the input prompts are shorter. \n",
    "    It formats the data entry as follows:\n",
    "\n",
    "    <user>\n",
    "    Identify the correct spelling of the following word: 'Occasion'\n",
    "\n",
    "    <assistant>\n",
    "    The correct spelling is 'Occasion'.\n",
    "    \"\"\" \n",
    "\n",
    "    instruction_text = (\n",
    "        f\"<|user|>\\n{entry['instruction']}\"\n",
    "    )\n",
    "\n",
    "    input_text = f\"\\n{entry['input']}\" if entry[\"input\"] else \"\"\n",
    "\n",
    "    return instruction_text + input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AmazonDatasetAlpaca(Dataset):\n",
    "    # Adjust Dataset to Alpaca template\n",
    "    def __init__(self, data, tokenizer, special_chars):\n",
    "        self.data = data\n",
    "\n",
    "        # Pre-tokenize texts\n",
    "        self.encoded_texts = []\n",
    "        for entry in data:\n",
    "            instruction_plus_input = format_input_alpaca(entry)\n",
    "            response_text = f\"\\n\\n### Response:\\n{entry['output']}\"\n",
    "            full_text = instruction_plus_input + response_text\n",
    "            self.encoded_texts.append(\n",
    "                tokenizer.encode(full_text, allowed_special=special_chars)\n",
    "            )\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.encoded_texts[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class AmazonDatasetPhi(Dataset):\n",
    "    # Adjust Dataset to Phi3 format\n",
    "    def __init__(self, data, tokenizer, special_chars):\n",
    "        self.data = data\n",
    "\n",
    "        # Pre-tokenize texts\n",
    "        self.encoded_texts = []\n",
    "        for entry in data:\n",
    "            instruction_plus_input = format_input_phi(entry)\n",
    "            response_text = f\"\\n<|assistant|>:\\n{entry['output']}\"\n",
    "            full_text = instruction_plus_input + response_text\n",
    "            self.encoded_texts.append(\n",
    "                tokenizer.encode(full_text, allowed_special=special_chars)\n",
    "            )\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.encoded_texts[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert dictionary to a more natural language sentence\n",
    "def details_to_sentence(details_dict):\n",
    "    sentences = [f\"{key.lower()} is: {value}\" for key, value in details_dict.items()]\n",
    "    return '. '.join(sentences) + '.'\n",
    "\n",
    "\n",
    "def dict_to_text(dict_data):\n",
    "  \"\"\"\n",
    "  Converts a dictionary of product details into a text string.\n",
    "\n",
    "  Args:\n",
    "    dict_data: A dictionary containing product details.\n",
    "\n",
    "  Returns:\n",
    "    A string representing the product details.\n",
    "  \"\"\"\n",
    "\n",
    "  text = \"\"\n",
    "  for key, value in dict_data.items():\n",
    "      text += f\"{key}: {value}\\n\"\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data_temporal(df, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15):\n",
    "    \"\"\"\n",
    "    Splits the data into train, validation, and test sets while preserving the temporal order of each user's interactions.\n",
    "\n",
    "    Args:\n",
    "    - df (pd.DataFrame): DataFrame containing user interactions with 'user_id' and 'date' columns.\n",
    "    - train_ratio (float): Proportion of data to be used for training.\n",
    "    - val_ratio (float): Proportion of data to be used for validation.\n",
    "    - test_ratio (float): Proportion of data to be used for testing.\n",
    "\n",
    "    Returns:\n",
    "    - train_df (pd.DataFrame): Training set.\n",
    "    - val_df (pd.DataFrame): Validation set.\n",
    "    - test_df (pd.DataFrame): Test set.\n",
    "    \"\"\"\n",
    "    assert train_ratio + val_ratio + test_ratio == 1.0, \"Ratios must sum up to 1.\"\n",
    "\n",
    "    # Sort data by user_id and date to maintain chronological order\n",
    "    df_sorted = df.sort_values(by=['input', 'date']).reset_index(drop=True)\n",
    "\n",
    "    # Create empty DataFrames for train, validation, and test\n",
    "    train_df = pd.DataFrame()\n",
    "    val_df = pd.DataFrame()\n",
    "    test_df = pd.DataFrame()\n",
    "\n",
    "    # Split each user's data chronologically\n",
    "    for user_id, group in df_sorted.groupby('input'):\n",
    "        num_interactions = len(group)\n",
    "\n",
    "        if num_interactions < 3:\n",
    "            # If a user has fewer than 3 interactions, skip them\n",
    "            continue\n",
    "\n",
    "        # Calculate indices for train, validation, and test splits\n",
    "        train_end = int(train_ratio * num_interactions)\n",
    "        val_end = train_end + int(val_ratio * num_interactions)\n",
    "\n",
    "        # Ensure there's at least one interaction in each split if possible\n",
    "        if train_end == 0:\n",
    "            train_end = 1\n",
    "        if val_end == train_end:\n",
    "            val_end = train_end + 1\n",
    "\n",
    "        # Concatenate each user's splits to the respective DataFrame\n",
    "        train_df = pd.concat([train_df, group.iloc[:train_end]])\n",
    "        val_df = pd.concat([val_df, group.iloc[train_end:val_end]])\n",
    "        test_df = pd.concat([test_df, group.iloc[val_end:]])\n",
    "\n",
    "    # Reset index after concatenation\n",
    "    train_df = train_df.reset_index(drop=True)\n",
    "    val_df = val_df.reset_index(drop=True)\n",
    "    test_df = test_df.reset_index(drop=True)\n",
    "\n",
    "    return train_df, val_df, test_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_10_items(df, padding_strategy='repeat', pad_token=\"<|endoftext|>\"):\n",
    "    \"\"\"\n",
    "    For each row in the dataframe, get the next 10 items the user interacted with, \n",
    "    while handling cases where less than 10 next items are available.\n",
    "\n",
    "    Args:\n",
    "    - df (pd.DataFrame): DataFrame containing user interactions, sorted by 'user_id' and 'date'.\n",
    "    - padding_strategy (str): How to pad the sequence if less than 10 items are found ('none', 'repeat', 'pad_token').\n",
    "    - pad_token (int): The padding token to use if padding_strategy is 'pad_token'.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: DataFrame with additional 'next_10_items' column.\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure the DataFrame is sorted by user_id and date for correct sequential order\n",
    "    df_sorted = df.sort_values(by=['user_id', 'date']).reset_index(drop=True)\n",
    "\n",
    "    # Initialize a dictionary to collect the new column data\n",
    "    next_10_items_data = []\n",
    "\n",
    "    # Group by each user\n",
    "    for user_id, user_data in df_sorted.groupby('user_id'):\n",
    "        user_data = user_data.reset_index(drop=True)\n",
    "        num_interactions = len(user_data)\n",
    "\n",
    "        # Skip users with fewer than 11 interactions\n",
    "        if num_interactions < 11:\n",
    "            continue\n",
    "\n",
    "        # Iterate over each row for the current user\n",
    "        for i in range(num_interactions):\n",
    "            if i + 10 < num_interactions:\n",
    "                # Get the next 10 item_ids\n",
    "                next_items = user_data.loc[i + 1:i + 10, 'item_id'].tolist()\n",
    "            else:\n",
    "                # Handle cases where there are fewer than 10 remaining items\n",
    "                next_items = user_data.loc[i + 1:, 'item_id'].tolist()\n",
    "                \n",
    "                if len(next_items) < 10:\n",
    "                    if padding_strategy == 'repeat':\n",
    "                        next_items = next_items + [next_items[-1]] * (10 - len(next_items))\n",
    "                    elif padding_strategy == 'pad_token':\n",
    "                        next_items = next_items + [pad_token] * (10 - len(next_items))\n",
    "                    elif padding_strategy == 'none':\n",
    "                        # Skip rows where next_10_items do not exist\n",
    "                        continue\n",
    "            \n",
    "            # Check if a list contains the same different or same item_ids all through the list\n",
    "            #if len(set(next_items)) > 1:\n",
    "\n",
    "            next_10_items_data.append((user_data.loc[i, 'user_id'],\n",
    "                                    user_data.loc[i, 'item_id'],\n",
    "                                    user_data.loc[i, 'date'],\n",
    "                                    \", \".join(next_items))\n",
    "                                    )\n",
    "\n",
    "    # Create a new DataFrame with the collected data\n",
    "    df_next_10_items = pd.DataFrame(next_10_items_data, columns=['user_id', 'item_id', 'date', 'next_10_items'])\n",
    "    return df_next_10_items\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(\n",
    "    batch,\n",
    "    pad_token_id=None, #end_of_text_id,\n",
    "    ignore_index=-100,\n",
    "    allowed_max_length=None, #1024,\n",
    "    device=\"cpu\"\n",
    "    ):\n",
    "    # Find the longest sequence in the batch\n",
    "    batch_max_length = max(len(item)+1 for item in batch)\n",
    "\n",
    "    # Pad and prepare inputs and targets\n",
    "    inputs_lst, targets_lst = [], []\n",
    "    for item in batch:\n",
    "        # Add an <|endoftext|> token\n",
    "        item += [pad_token_id]\n",
    "        # Pad sequences to max_length\n",
    "        padded = item + [pad_token_id] * (batch_max_length - len(item))\n",
    "        inputs = torch.tensor(padded[:-1])  # Truncate the last token for inputs\n",
    "        targets = torch.tensor(padded[1:])  # Shift +1 to the right for targets\n",
    "\n",
    "        # Replace all but the first padding tokens in targets by ignore_index\n",
    "        mask = targets == pad_token_id\n",
    "        indices = torch.nonzero(mask).squeeze()\n",
    "        if indices.numel() > 1:\n",
    "            targets[indices[1:]] = ignore_index\n",
    "\n",
    "        # Optionally truncate to maximum sequence length\n",
    "        if allowed_max_length is not None:\n",
    "            inputs = inputs[:allowed_max_length]\n",
    "            targets = targets[:allowed_max_length]\n",
    "\n",
    "        inputs_lst.append(inputs)\n",
    "        targets_lst.append(targets)\n",
    "\n",
    "    inputs_tensor = torch.stack(inputs_lst).to(device)\n",
    "    targets_tensor = torch.stack(targets_lst).to(device)\n",
    "\n",
    "    return inputs_tensor, targets_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and Join data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_reviews_df = pd.DataFrame(load_json_data(PATH_RAW_DATA))\n",
    "data_meta_df = pd.DataFrame(load_json_data(PATH_META_DATA))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join user and item metadata\n",
    "data_reviews_df.drop(columns=[\"images\", \"title\"], inplace=True)\n",
    "full_df = data_reviews_df.merge(data_meta_df, on=[\"parent_asin\"], how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Reviews data shape: {data_reviews_df.shape}\")\n",
    "print(f\"Meta data shape: {data_meta_df.shape}\")\n",
    "print(f\"Joined data shape: {full_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data_meta_df, data_reviews_df\n",
    "print(full_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data filters:\n",
    "- verified_purchase is True: select only users that are verified that purchased the item\n",
    "- drop columns that are not needed due to the context length of 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_cols = [\n",
    "    # User reviews data\n",
    "    \"rating\", \"text\", \"parent_asin\", \"user_id\", \"timestamp\",\n",
    "    # Product meta data\n",
    "    'main_category', 'title', 'details', 'price', \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df_selected = full_df[full_df[\"verified_purchase\"] == True]\n",
    "full_df_selected = full_df_selected[selected_cols]\n",
    "\n",
    "del full_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data user and item to custom format\n",
    "Prepare the data by setting the user_id and item_id to a special token format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df_selected.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to the DataFrame column\n",
    "full_df_selected[\"date\"] = cast_unix_to_date(full_df_selected[\"timestamp\"])\n",
    "full_df_selected = full_df_selected.sort_values(by=[\"date\", \"user_id\"])\n",
    "\n",
    "full_df_selected[\"user_id\"]= \"<|user_\" + full_df_selected[\"user_id\"] + \"|>\"\n",
    "full_df_selected[\"item_id\"] = \"<|item_\" + full_df_selected[\"parent_asin\"] + \"|>\"\n",
    "\n",
    "full_df_selected['item_details'] = full_df_selected['details'].apply(details_to_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract 10 items purchased by the user\n",
    "Extract the next 10 items purchased by the user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model = get_next_10_items(full_df_selected, 'pad_token')\n",
    "print(\"Min:\", df_model[\"date\"].min(), \"\\tMax:\", df_model[\"date\"].max())\n",
    "print(\"Shape:\", df_model.shape)\n",
    "df_model.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "join_keys = [\"user_id\", \"item_id\", \"date\"]\n",
    "\n",
    "tmp_df = full_df_selected.drop([\"timestamp\", \"details\", \"parent_asin\"], axis=1).drop_duplicates().reset_index(drop=True)\n",
    "df_model_ready = df_model.merge(tmp_df, on=join_keys, how=\"inner\")\n",
    "\n",
    "print(\"Dataframe shape is consistent\", df_model_ready.shape[0] == df_model.shape[0])\n",
    "\n",
    "del df_model, tmp_df, full_df_selected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rename the columns to descriptive names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_cols_names = {\n",
    "    \"rating\": \"Rating of the item by user from 1 to 5\",\n",
    "    \"text\": \"Text of the user review\",\n",
    "\n",
    "    \"main_category\": \"Main category of the item\",\n",
    "    'title': \"Item name\", \n",
    "\n",
    "    'item_details': \"Item details\",\n",
    "    'item_id': 'Item id',\n",
    "    'price': 'Price USD'\n",
    "}\n",
    "\n",
    "df_model_ready = df_model_ready.rename(columns=new_cols_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the list of special tokens\n",
    "Let the identify user_id, item and \"<|endoftext|>\" as additional tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_user_item_ids = [\"<|endoftext|>\"] + df_model_ready[\"user_id\"].unique().tolist() + df_model_ready[\"Item id\"].unique().tolist()\n",
    "print(special_user_item_ids[:3])\n",
    "special_user_item_ids = set(special_user_item_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify the token assigned to \"<|endoftext|>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")   \n",
    "#tokenizer = tiktoken.get_encoding(\"o200k_base\")\n",
    "\n",
    "end_of_text_id = np.array(tokenizer.encode(\"<|endoftext|>\", allowed_special=special_user_item_ids)).item()\n",
    "print(end_of_text_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning LLM process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation - input, output and instruction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create input and output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_ready.rename(columns={\"user_id\": \"input\", 'next_10_items': \"output\"}, inplace=True)\n",
    "\n",
    "\n",
    "aux_cols =[\n",
    "            'Item id',\n",
    "            'Rating of the item by user from 1 to 5',\n",
    "            'Text of the user review',\n",
    "            'Main category of the item',\n",
    "            'Item name',\n",
    "            'Price USD',\n",
    "            'Item details'\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert integer columns to string\n",
    "df_model_ready['Rating of the item by user from 1 to 5'] = df_model_ready['Rating of the item by user from 1 to 5'].map(str)\n",
    "df_model_ready['Price USD'] = df_model_ready['Price USD'].map(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The validation and test data should not have item details, just input and output.\n",
    "Write a function that takes into account data split type `train`, `test` and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "user_question = \"Given a user purchased an an item with the following details, predict the next 10 items the user would  purchase. \"\n",
    "df_model_ready[\"instruction\"] =  user_question + df_model_ready[aux_cols].apply(lambda x: '. '.join(f\"{col} is {value}\" for col, value in x.items()), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split to train, test and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "train_df, val_df, test_df = split_data_temporal(df_model_ready, train_ratio=0.85, val_ratio=0.1, test_ratio=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df= train_df[[\"input\", \"output\", \"instruction\"]]\n",
    "val_df= val_df[[\"input\", \"output\", \"instruction\"]]\n",
    "test_df= test_df[[\"input\", \"output\", \"instruction\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_df.to_dict(orient=\"records\")\n",
    "test_data = test_df.to_dict(orient=\"records\")\n",
    "val_data = val_df.to_dict(orient=\"records\")\n",
    "\n",
    "del train_df, val_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SELECT_USER_TEMPLATE == \"Phi3\":\n",
    "    model_input = format_input_phi(train_data[30])\n",
    "elif SELECT_USER_TEMPLATE == \"Alpaca\":\n",
    "    model_input = format_input_alpaca(train_data[30])\n",
    "\n",
    "desired_response = f\"\\n<|assistant|>:\\n{train_data[30]['output']}\"\n",
    "print(model_input + desired_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training set length:\", len(train_data))\n",
    "print(\"Validation set length:\", len(val_data))\n",
    "print(\"Test set length:\", len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 0\n",
    "batch_size = 4\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "if SELECT_USER_TEMPLATE == \"Phi3\":\n",
    "    train_dataset = AmazonDatasetPhi(train_data, tokenizer, special_user_item_ids)\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        #collate_fn=customized_collate_fn,\n",
    "        collate_fn=lambda x: custom_collate_fn(x, pad_token_id=end_of_text_id, device=device),\n",
    "        shuffle=True,\n",
    "        drop_last=True\n",
    "    )\n",
    "\n",
    "\n",
    "else:\n",
    "    train_dataset = AmazonDatasetAlpaca(train_data, tokenizer, special_user_item_ids)\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        #collate_fn=customized_collate_fn,\n",
    "        collate_fn=lambda x: custom_collate_fn(x, pad_token_id=end_of_text_id, device=device),\n",
    "        shuffle=True,\n",
    "        drop_last=True\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SELECT_USER_TEMPLATE == \"Phi3\":\n",
    "    val_dataset = AmazonDatasetPhi(val_data, tokenizer, special_user_item_ids)\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        #collate_fn=customized_collate_fn,\n",
    "        collate_fn=lambda x: custom_collate_fn(x, pad_token_id=end_of_text_id,  device=device),\n",
    "        shuffle=False,\n",
    "        drop_last=False\n",
    "    )\n",
    "\n",
    "    test_dataset = AmazonDatasetPhi(test_data, tokenizer, special_user_item_ids)\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        #collate_fn=customized_collate_fn,\n",
    "        collate_fn=lambda x: custom_collate_fn(x, pad_token_id=end_of_text_id, device=device),\n",
    "        shuffle=False,\n",
    "        drop_last=False\n",
    "    )\n",
    "\n",
    "else:\n",
    "    val_dataset = AmazonDatasetAlpaca(val_data, tokenizer, special_user_item_ids)\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        #collate_fn=customized_collate_fn,\n",
    "        collate_fn=lambda x: custom_collate_fn(x, pad_token_id=end_of_text_id, device=device),\n",
    "        shuffle=False,\n",
    "        drop_last=False\n",
    "    )\n",
    "\n",
    "    test_dataset = AmazonDatasetAlpaca(test_data, tokenizer, special_user_item_ids)\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        #collate_fn=customized_collate_fn,\n",
    "        collate_fn=lambda x: custom_collate_fn(x, pad_token_id=end_of_text_id, device=device),\n",
    "        shuffle=False,\n",
    "        drop_last=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading a pretrained LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_CONFIG = {\n",
    "    \"vocab_size\": 50257, #50257,     # Vocabulary size\n",
    "    \"context_length\": 1024,  # Context length\n",
    "    \"drop_rate\": 0.0,        # Dropout rate\n",
    "    \"qkv_bias\": True         # Query-key-value bias\n",
    "}\n",
    "\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "CHOOSE_MODEL = \"gpt2-small (124M)\"\n",
    "\n",
    "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
    "\n",
    "model_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n",
    "settings, params = download_and_load_gpt2(model_size=model_size, models_dir=\"gpt2\")\n",
    "\n",
    "model = GPTModel(BASE_CONFIG)\n",
    "load_weights_into_gpt(model, params)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning the LLM on reviews data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "\n",
    "torch.manual_seed(123)  # For reproducibility due to the shuffling in the data loader\n",
    "\n",
    "with torch.no_grad():  # Disable gradient tracking for efficiency because we are not training, yet\n",
    "    train_loss = calc_loss_loader(train_loader, model, device, num_batches=3)\n",
    "    val_loss = calc_loss_loader(val_loader, model, device, num_batches=3)\n",
    "\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Percentage change in loss:\", round(100 * (val_loss - train_loss)/train_loss, 2), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0005, weight_decay=0.1)\n",
    "\n",
    "num_epochs = 2\n",
    "\n",
    "if SELECT_USER_TEMPLATE == \"Phi3\":\n",
    "    train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "        model, train_loader, val_loader, optimizer, device,\n",
    "        num_epochs=num_epochs, eval_freq=3, eval_iter=3,\n",
    "        start_context=format_input_phi(val_data[0]), tokenizer=tokenizer,\n",
    "        special_chars=special_user_item_ids\n",
    "    )\n",
    "else:\n",
    "    train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "        model, train_loader, val_loader, optimizer, device,\n",
    "        num_epochs=num_epochs, eval_freq=3, eval_iter=3,\n",
    "        start_context=format_input_alpaca(val_data[0]), tokenizer=tokenizer,\n",
    "        special_chars=special_user_item_ids\n",
    "    )\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting and saving responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_response_alpaca(response):\n",
    "    return response[response.find(\"\\n### Response\")+len(\"\\n### Response:\")+1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for entry in test_data[:3]:\n",
    "\n",
    "    if SELECT_USER_TEMPLATE == \"Phi3\":\n",
    "        input_text = format_input_phi(entry):\n",
    "    else:\n",
    "        input_text = format_input_alpaca(entry)\n",
    "\n",
    "    token_ids = generate(\n",
    "        model=model,\n",
    "        idx=text_to_token_ids(input_text, tokenizer, special_user_item_ids).to(device),\n",
    "        max_new_tokens=256,\n",
    "        context_size=BASE_CONFIG[\"context_length\"],\n",
    "        eos_id=end_of_text_id\n",
    "    )\n",
    "    response = token_ids_to_text(token_ids, tokenizer)\n",
    "\n",
    "    if SELECT_USER_TEMPLATE == \"Phi3\":\n",
    "        response_text = response[len(input_text):].replace(\"<|assistant|>:\", \"\").strip()\n",
    "    else:\n",
    "        response_text = response[len(input_text):].replace(\"### Response:\", \"\").strip()\n",
    "\n",
    "    print(input_text)\n",
    "    print(f\"\\nCorrect response:\\n>> {entry['output']}\")\n",
    "    print(f\"\\nModel response:\\n>> {response_text.strip()}\")\n",
    "    print(\"-------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, entry in tqdm(enumerate(test_data), total=len(test_data)):\n",
    "\n",
    "    if SELECT_USER_TEMPLATE == \"Phi3\":\n",
    "        input_text = format_input_phi(entry):\n",
    "    else:\n",
    "        input_text = format_input_alpaca(entry)\n",
    "\n",
    "    token_ids = generate(\n",
    "        model=model,\n",
    "        idx=text_to_token_ids(input_text, tokenizer, special_user_item_ids).to(device),\n",
    "        max_new_tokens=256,\n",
    "        context_size=BASE_CONFIG[\"context_length\"],\n",
    "        eos_id=end_of_text_id\n",
    "    )\n",
    "    response = token_ids_to_text(token_ids, tokenizer)\n",
    "\n",
    "    if SELECT_USER_TEMPLATE == \"Phi3\":\n",
    "        response_text = response[len(input_text):].replace(\"<|assistant|>:\", \"\").strip()\n",
    "    else:\n",
    "        response_text = response[len(input_text):].replace(\"### Response:\", \"\").strip()\n",
    "\n",
    "    test_data[i][\"model_response\"] = response_text\n",
    "\n",
    "\n",
    "with open(\"data/output/recsys-data-with-response.json\", \"w\") as file:\n",
    "    json.dump(test_data, file, indent=4)  # \"indent\" for pretty-printing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "file_name = f\"output/model/{re.sub(r'[ ()]', '', CHOOSE_MODEL) }-sft.pth\"\n",
    "torch.save(model.state_dict(), file_name)\n",
    "print(f\"Model saved as {file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#python gpt-experiment.py --run_solution phi3_prompt"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
