{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User path config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"data/output/amazon-reviews-data-with-response-phi3-prompt.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_metrics(output, model_response, k=10):\n",
    "    \"\"\"\n",
    "    Evaluate model's recommendation performance using precision@k, recall@k, and MRR.\n",
    "    \n",
    "    Args:\n",
    "        output (str): The ground truth output containing the actual items (comma-separated string).\n",
    "        model_response (str): The predicted items by the model (comma-separated string).\n",
    "        k (int): The number of top items to consider for precision@k, recall@k, etc.\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary containing precision@k, recall@k, and MRR.\n",
    "    \"\"\"\n",
    "    # Parse the output and model_response strings into lists\n",
    "    actual_items = [item.strip() for item in output.split(\",\") if \"item_\" in item]\n",
    "    predicted_items = [item.strip() for item in model_response.split(\",\") if \"item_\" in item][:k]  # Only consider top-K predictions\n",
    "\n",
    "    # Remove duplicates in predictions (since duplicates don't count multiple times)\n",
    "    predicted_items = list(dict.fromkeys(predicted_items))\n",
    "\n",
    "    # Calculate true positives\n",
    "    true_positives = set(actual_items) & set(predicted_items)\n",
    "    \n",
    "    # Calculate precision@k (How many of the top-K predicted are correct)\n",
    "    precision_at_k = len(true_positives) / min(len(predicted_items), k)\n",
    "\n",
    "    # Calculate recall@k (How many of the relevant items are found in the top-K)\n",
    "    recall_at_k = len(true_positives) / len(actual_items) if len(actual_items) > 0 else 0\n",
    "\n",
    "    # Calculate MRR (Mean Reciprocal Rank)\n",
    "    mrr = 0\n",
    "    for rank, predicted_item in enumerate(predicted_items, 1):\n",
    "        if predicted_item in actual_items:\n",
    "            mrr = 1 / rank\n",
    "            break\n",
    "\n",
    "    # Compile metrics into a dictionary\n",
    "    metrics = {\n",
    "        \"precision@{}\".format(k): precision_at_k,\n",
    "        \"recall@{}\".format(k): recall_at_k,\n",
    "        \"mrr\": mrr\n",
    "    }\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_metrics_on_list(data_list, k=10):\n",
    "    \"\"\"\n",
    "    Evaluate precision@k, recall@k, and MRR on a list of data.\n",
    "    \n",
    "    Args:\n",
    "        data_list (list): A list of dictionaries containing 'output' and 'model_response'.\n",
    "        k (int): The number of top items to consider for precision@k, recall@k, etc.\n",
    "        \n",
    "    Returns:\n",
    "        dict: Average precision@k, recall@k, and MRR for the entire dataset.\n",
    "    \"\"\"\n",
    "    precision_sum = 0\n",
    "    recall_sum = 0\n",
    "    mrr_sum = 0\n",
    "    num_samples = len(data_list)\n",
    "\n",
    "    for data in data_list:\n",
    "        output = data[\"output\"]\n",
    "        model_response = data[\"model_response\"]\n",
    "\n",
    "        # Calculate metrics for each entry\n",
    "        metrics = evaluate_metrics(output, model_response, k)\n",
    "        \n",
    "        # Accumulate the metrics\n",
    "        precision_sum += metrics[f\"precision@{k}\"]\n",
    "        recall_sum += metrics[f\"recall@{k}\"]\n",
    "        mrr_sum += metrics[\"mrr\"]\n",
    "\n",
    "    # Calculate the average metrics over all samples\n",
    "    avg_metrics = {\n",
    "        f\"precision@{k}\": precision_sum / num_samples,\n",
    "        f\"recall@{k}\": recall_sum / num_samples,\n",
    "        \"mrr\": mrr_sum / num_samples\n",
    "    }\n",
    "\n",
    "    return avg_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_endoftext_items(data_list):\n",
    "    filtered_data = []\n",
    "    \n",
    "    for entry in data_list:\n",
    "        # Check if all tokens in 'output' are '<|endoftext|>'\n",
    "        output_items = [item.strip() for item in entry['output'].split(',')]\n",
    "        \n",
    "        # If the output is not all '<|endoftext|>', keep the entry\n",
    "        if any(item != '<|endoftext|>' for item in output_items):\n",
    "            filtered_data.append(entry)\n",
    "    \n",
    "    return filtered_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open and read the JSON file\n",
    "with open(DATA_PATH, 'r') as f:\n",
    "    data_list = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list_with_atleast_one_item_output = filter_endoftext_items(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original test data: 627\n",
      "Filtered test data with at least one output item: 230\n"
     ]
    }
   ],
   "source": [
    "print(\"Original test data:\", len(data_list))\n",
    "print(\"Filtered test data with at least one output item:\", len(data_list_with_atleast_one_item_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metrics: {'precision@10': 0.008040302777144884, 'recall@10': 0.01967038809144072, 'mrr': 0.015151515151515152}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the metrics\n",
    "avg_metrics = evaluate_metrics_on_list(data_list, k=10)\n",
    "print(\"Average Metrics:\", avg_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exclude materials from test data\n",
    "Exclude materials that do not have any item in the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metrics: {'precision@10': 0.02191856452726018, 'recall@10': 0.0536231884057971, 'mrr': 0.041304347826086954}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the metrics\n",
    "avg_metrics = evaluate_metrics_on_list(data_list_with_atleast_one_item_output, k=10)\n",
    "print(\"Average Metrics:\", avg_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': '<|user_AFIGEAZMENNV67CZRJH5P66MNIWA|>',\n",
       " 'output': '<|item_B07PJ8VDSP|>, <|endoftext|>, <|endoftext|>, <|endoftext|>, <|endoftext|>, <|endoftext|>, <|endoftext|>, <|endoftext|>, <|endoftext|>, <|endoftext|>',\n",
       " 'instruction': \"Given a user purchased an an item with the following details, predict the next 10 items the user would  purchase. Item id is <|item_B07TX13LTP|>. Rating of the item by user from 1 to 5 is 1.0. Text of the user review is Short life span . dont borther. Main category of the item is Appliances. Item name is DIKOO WR51X10101 Heater Harness Defrost Assembly Compatible for General Electric Refrigerators Replaces AP4355467, 1399613,EA1993872, WR51X10053. Price USD is 13.29. Item details is brand name is: DIKOO. model info is: WR51X10101DIK. item weight is: 7.8 ounces. package dimensions is: 9.96 x 3.98 x 3.43 inches. item model number is: WR51X10101DIK. part number is: WR51X10101DIK. form factor is: Compact. batteries included? is: No. batteries required? is: No. best sellers rank is: {'Tools & Home Improvement': 221564, 'Freezer Parts & Accessories': 212}. date first available is: July 6, 2019. brand is: DIKOO. pattern is: Solid.\",\n",
       " 'model_response': '<|item_B07TX13LTP|>, <|item_B07PJ8H3XJ|>, <|item_B07PJ8H3XJ|>, <|item_B07PJ8H3XJ|>, <|item_B07PJ8H3XJ|>, <|item_B07PJ8H3XJ|>, <|item_B07PJ8H3XJ|>, <|item_B07PJ8H3XJ|>, <|item_B07H3J23LJ|>, <|item_B07H3J23LJ|>'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_list_with_atleast_one_item_output[78]"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
